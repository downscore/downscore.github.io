---
title: "Meta-Interpretability"
date: 2025-11-28
---

Mechanistic interpretability research isn't very interpretable for a lot of people.

AI-years are like dog years, right? If so, then around a decade ago, a Zvi wrote an ["Easily Interpretable Summary"](https://thezvi.substack.com/p/i-am-the-golden-gate-bridge) of Anthropic's interpretability paper on [Scaling Monosemanticity](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html), also known as the "Golden Gate Claude" paper.

It's easy to find AI-generated summaries of AI-interpretability papers online. In fact, it's hard to find much else. These are just obviously worse than what you could get by prompting your favourite frontier model yourself (unless they included some deep insight in their prompt, I guess).

So why is Zvi's summary better than an AI summary?

First of all, it's around 8000 words. The paper is quite long, but this definitely isn't just a lossy compression of its content. It contains the full chain of thought of a human trying to understand the paper, and it injects a lot of human perspective into the process. I generally like humans, so I think all of this is really valuable. It's also a sharp contrast to the default "just the facts, some of which are fabrications" approach of AI summaries.

I also found the summary inspiring as a way of working through complicated material. The main reason I started this blog was to help track my progress in studying various topics, including interpretability. I figure that putting things in writing will help keep me honest about what I do and don't understand. Putting it all on a world-readable blog may help further.

Regarding the name of the blog: I will be doing all this with the unbounded energy of an illiterate goofball in the aughts hitting Yahoo Answers like a ton of bricks.
